import sqlalchemy
import pymysql
import pandas as pd
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
import logging

# Configurer la journalisation
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')

def retrain_model():
    logging.info("D√©but du r√©entra√Ænement du mod√®le")
    try:
        # üîπ Connexion √† la base de donn√©es MySQL
        logging.info("Connexion √† la base de donn√©es MySQL...")
        engine = sqlalchemy.create_engine('mysql+pymysql://user:userpassword@localhost/tweets_db')
        logging.info("Connexion r√©ussie √† la base de donn√©es MySQL")

        query = "SELECT text, positive, negative FROM tweets"
        df = pd.read_sql(query, engine)
        logging.info("Donn√©es r√©cup√©r√©es depuis la base de donn√©es MySQL")
        logging.info(f"Nombre de lignes r√©cup√©r√©es : {len(df)}")

        # üîπ Fonction de nettoyage du texte
        def clean_text(text):
            text = text.lower()  # Minuscule
            text = re.sub(r'[^\w\s]', '', text)  # Suppression des caract√®res sp√©ciaux
            return text

        df['text_clean'] = df['text'].apply(clean_text)
        logging.info("Nettoyage du texte termin√©")

        # V√©rifier la distribution des classes avant la s√©paration des donn√©es
        logging.info(f"Distribution des classes (positif) avant la s√©paration : {df['positive'].value_counts().to_dict()}")
        logging.info(f"Distribution des classes (n√©gatif) avant la s√©paration : {df['negative'].value_counts().to_dict()}")

        # üîπ Stopwords fran√ßais (√† filtrer dans la vectorisation)
        french_stopwords = [
            "le", "la", "les", "un", "une", "des", "du", "de", "dans", "et", "en", "au",
            "aux", "avec", "ce", "ces", "pour", "par", "sur", "pas", "plus", "o√π", "mais",
            "ou", "donc", "ni", "car", "ne", "que", "qui", "quoi", "quand", "√†", "son",
            "sa", "ses", "ils", "elles", "nous", "vous", "est", "sont", "cette", "cet",
            "aussi", "√™tre", "avoir", "faire", "comme", "tout", "bien", "mal", "on", "lui"
        ]

        vectorizer = CountVectorizer(stop_words=french_stopwords, max_features=100)
        X = vectorizer.fit_transform(df['text_clean'])
        y_positive = df['positive']
        y_negative = df['negative']

        # üîπ S√©paration des donn√©es en train/test
        X_train_pos, X_test_pos, y_train_positive, y_test_positive = train_test_split(X, y_positive, test_size=0.25, random_state=42)
        X_train_neg, X_test_neg, y_train_negative, y_test_negative = train_test_split(X, y_negative, test_size=0.25, random_state=42)

        # V√©rifier la distribution des classes apr√®s la s√©paration des donn√©es
        logging.info(f"Distribution des classes dans y_train_positive : {pd.Series(y_train_positive).value_counts().to_dict()}")
        logging.info(f"Distribution des classes dans y_train_negative : {pd.Series(y_train_negative).value_counts().to_dict()}")

        # V√©rifier si les classes sont √©quilibr√©es
        if len(y_train_positive.unique()) < 2 or len(y_train_negative.unique()) < 2:
            logging.error("Les donn√©es d'entra√Ænement ne contiennent pas suffisamment de classes pour l'entra√Ænement.")
            return

        # üîπ Entra√Ænement du mod√®le de r√©gression logistique pour les tweets positifs
        model_positive = LogisticRegression()
        model_positive.fit(X_train_pos, y_train_positive)

        # üîπ Entra√Ænement du mod√®le de r√©gression logistique pour les tweets n√©gatifs
        model_negative = LogisticRegression()
        model_negative.fit(X_train_neg, y_train_negative)

        # üîπ Pr√©dictions et √©valuation du mod√®le pour les tweets positifs
        y_pred_positive = model_positive.predict(X_test_pos)
        logging.info("Rapport de classification (positif) :")
        logging.info("\n" + classification_report(y_test_positive, y_pred_positive))

        logging.info("Matrice de confusion (positif) :")
        logging.info("\n" + str(confusion_matrix(y_test_positive, y_pred_positive)))

        # üîπ Pr√©dictions et √©valuation du mod√®le pour les tweets n√©gatifs
        y_pred_negative = model_negative.predict(X_test_neg)
        logging.info("Rapport de classification (n√©gatif) :")
        logging.info("\n" + classification_report(y_test_negative, y_pred_negative))

        logging.info("Matrice de confusion (n√©gatif) :")
        logging.info("\n" + str(confusion_matrix(y_test_negative, y_pred_negative)))

    except Exception as e:
        logging.error("Erreur lors du r√©entra√Ænement du mod√®le : %s", e)

if __name__ == "__main__":
    retrain_model()